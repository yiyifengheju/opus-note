---
title: 半监督学习
date: 2024-12-30
comments: true
---

对于只有有限标签数据的有监督任务，通常有四种候选方案[^1]：

1. 预训练+微调 (Pre-Training+Fine-Tuning)。在大规模数据上训练一个与任务无关 (Task-Agnostic)
   的模型，然后在下游任务中通过少量的有标签样本集合对模型进行微调。
2. 半监督学习 (Semi-Supervised Learning)。在有标签数据和无标签样本上共同学习。
3. 主动学习 (Active Learning)。选择最有价值的无标签样本，在有限成本下对这些样本打标签。
4. 预训练 + 数据集自生成 (Pre-Training + Dataset Auto-Generation)
   ：给定一个良好的预训练模型，利用它生成更多的有标签样本。受少样本学习 (Few-Shot Learning)的启发，这种方式在语言领域很流行。

## 壹丨概念

定义：半监督学习通过使用有标签和无标签数据来共同训练模型

常用损失函数：
$$
L = L_s+\mu(t)L_u
$$
加权项$\mu(t)$一般选择斜坡函数，无监督损失随时间增加

半监督学习假设：

* H1：平滑假设 —— 如果两个数据样本在特征空间中的高密度区域非常接近，则它们的标签应相同或相似。
* H2：聚类假设——特征空间既有密集区域，也有稀疏区域。位于密集区域的数据点会自然地会形成簇，而同一簇中的样本应具有相同的标签。这是聚类假设的一个小扩展。
* H3：低密度分离假设——类之间的决策边界往往位于稀疏、低密度区域，否则决策边界会将高密度簇分为两类，从而形成两个簇，使得平滑假设和聚类假设失效。
*
H4：流形假设——高维数据实际上是由一个低维流形映射到高维空间上的。即使真实世界中的数据是在高维度上观察到的，例如真实世界中的物体/场景的图片，我们也可以捕获它们的内在低维流形结构。在这里就可以发现数据的潜在规律，相似的数据点将被聚到一起 (
真实世界中的物体/场景的图像不是在所有像素组合中均匀采样的)。这使得我们能够学习一个更有效地表示，来度量无标签数据之间的相似性。这也是表示学习的基础

## 贰丨方法

一致性正则化 (Consistency Regularization)，又称一致性训练 (Consistency Training)
，假设在给定相同输入的情况下，神经网络内的随机性 (如Dropout)或数据增强变换不会改变模型的预测结果。

应用：SimCLR、BYOL、SimCSE等

!!! note "自监督学习核心思想"

	同一样本的不同增强版本应产生相同的表示

### 1. Pi-Model

https://mp.weixin.qq.com/s/Uea0c-PP2csckT_s8wOH-A

定义：当同一数据点经过不同的随机变换 (如Dropout、随机最大池化等)，将得到两个版本 (Version)作为网络的输入，我们期望它们对应的输出是一致的。

缺点：网络对每个样本运行两次，计算成本高

### 2. Temporal Ensembling

每个样本的EMA标签预测是模型的学习目标。

[^1]: 1 Lil'Log，@ Lilian
Weng，[Learning with not Enough Data Part 1: Semi-Supervised Learning](https://lilianweng.github.io/posts/2021-12-05-semi-supervised/)
↩