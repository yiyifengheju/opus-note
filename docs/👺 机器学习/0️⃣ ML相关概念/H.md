---
title: 信息熵：度量不确定性
date: 2024.10.20
comments: true
---

!!! warning "以下内容由AI生成"

在信息论中，信息熵（Information Entropy）是一个核心概念，用于度量信息的不确定性或随机性。信息熵由克劳德·香农（Claude Shannon）在1948年提出，因此也被称为香农熵（Shannon Entropy）。

### 1. 信息熵定义

信息熵是一个随机变量的不确定性的度量。对于一个离散随机变量 $X$，其可能取值为 $\{x_1, x_2, \ldots, x_n\}$，对应的概率为 $\{p_1, p_2, \ldots, p_n\}$，信息熵 $H(X)$ 定义为：


$$
H(X) = -\sum_{i=1}^{n} p(x_i) \log_2 p(x_i)
$$


其中：

- $p(x_i)$ 是随机变量 $X$ 取值为 $x_i$ 的概率。
- $\log_2$ 是以2为底的对数。

!!! note "直观理解"

    信息熵度量了一个随机变量的平均信息量。信息熵越大，表示不确定性越高，信息量越大；信息熵越小，表示不确定性越低，信息量越小。
    
    举个例子，假设有一个二进制随机变量 $X$，其可能取值为 $\{0, 1\}$，对应的概率为 $\{0.5, 0.5\}$。则其信息熵为：
    
    
    $$
    H(X) = -0.5 \log_2 0.5 - 0.5 \log_2 0.5 = 1
    $$
    
    
    这表示在这种情况下，信息的不确定性是最大的。

### 2. 信息熵的特性

1. **非负性**：信息熵总是非负的，即 $H(X) \geq 0$。
2. **最大值**：当所有可能的取值均匀分布时，信息熵达到最大值。
3. **最小值**：当随机变量是确定的（即只有一个可能的取值，概率为1）时，信息熵为0。

### 3. 信息熵的应用

信息熵在许多领域有广泛的应用，包括但不限于：

- **数据压缩**：信息熵用于度量数据的最小编码长度。通过计算信息熵，可以确定数据的最小表示形式，从而实现高效的数据压缩。
- **机器学习**：在决策树算法中，信息熵用于选择最优分裂点。通过计算每个特征的信息熵，可以找到最能区分数据的特征，从而构建更有效的决策树。
- **通信理论**：信息熵用于度量信道容量和信息传输效率。通过计算信道的最大信息熵，可以确定其最大传输速率，从而优化通信系统的设计。

### 4. 计算示例

```python
import numpy as np

def entropy(probabilities):
    return -np.sum(probabilities * np.log2(probabilities))

# 示例：二进制随机变量，概率为0.5和0.5
probabilities = np.array([0.5, 0.5])
print("信息熵:", entropy(probabilities))

# 示例：三进制随机变量，概率为0.2, 0.3和0.5
probabilities = np.array([0.2, 0.3, 0.5])
print("信息熵:", entropy(probabilities))
```
